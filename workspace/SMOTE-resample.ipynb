{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f91fd6e-dc2a-43e0-a9ba-9d2d9e6bac24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Obtaining dependency information for imbalanced-learn from https://files.pythonhosted.org/packages/a3/9e/fbe60a768502af54563dcb59ca7856f5a8833b3ad5ada658922e1ab09b7f/imbalanced_learn-0.11.0-py3-none-any.whl.metadata\n",
      "  Downloading imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (3.2.0)\n",
      "Downloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.6/235.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.11.0\n"
     ]
    }
   ],
   "source": [
    "# install imblearn to leverage SMOTE resample\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed9e3e5-f094-41c1-9b53-eebdab97026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some relevant libraries\n",
    "from IPython.display import display\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    " \n",
    "sns.set()\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2833b8-0763-462d-9418-2e30c074c297",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Load the underlying dataset and do some quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efec0d3b-26d2-47b0-8217-6bf45c3ce25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset name\n",
    "## NOTE replace this with new dataset\n",
    "dataset_name = \"OTPW_3M\" #\"OTPW_3M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91a21fbc-ecce-47c5-b1a5-dfaa79d55d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|YEAR|  count|\n",
      "+----+-------+\n",
      "|2015|1401363|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ingest flights dataset\n",
    "# NOTEO edit this to point to new data\n",
    "df_flights = spark.read.format('parquet')\\\n",
    "                        .option(\"inferSchema\", True)\\\n",
    "                        .load(f\"../data/{dataset_name}\").cache()\n",
    "\n",
    "# Group data by year and count records for each year\n",
    "year_counts = df_flights.groupBy('YEAR').count()\n",
    " \n",
    "# Show the count of records for each year\n",
    "year_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a6fc90e-9664-4da7-9228-8fdef1088edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Flights Delayed in 60M: 277302\n",
      "Total Number of Flights in 60M: 1401363\n",
      "Percentage of Flights Delayed in 60M: 19.79%\n"
     ]
    }
   ],
   "source": [
    "# how many delays per total set\n",
    "print(f\"Number of Flights Delayed in 60M: {df_flights.filter('DEP_DEL15 = 1').count()}\")\n",
    "print(f\"Total Number of Flights in 60M: {df_flights.count()}\")\n",
    "print(f\"Percentage of Flights Delayed in 60M: {round(df_flights.filter('DEP_DEL15 = 1').count()/df_flights.count()*100,2)}%\")\n",
    " \n",
    "majority_class_proba = 1-round(df_flights.filter('DEP_DEL15 = 1').count()/df_flights.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abdf705-6546-4816-b1c8-5c0d3d9742fb",
   "metadata": {},
   "source": [
    "# Select and Clean Some Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f25e5d5e-886b-4e8c-98a9-56fcb78aaab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Flights (Non Null): 1088005\n"
     ]
    }
   ],
   "source": [
    "## Weather features\n",
    "numerics_features = [\"HourlyPrecipitation\", \"HourlyDryBulbTemperature\", \n",
    "                     \"HourlyRelativeHumidity\", \"HourlySeaLevelPressure\", \n",
    "                     \"HourlyStationPressure\", \"HourlyVisibility\",\n",
    "                     \"HourlyWindSpeed\", \"ELEVATION\"]\n",
    " \n",
    "## Cast Column to Float\n",
    "for c in numerics_features:\n",
    "    df_flights = df_flights.withColumn(c, F.col(c).cast(\"Float\"))\n",
    " \n",
    "# filter out Null values from our dataset (12M)\n",
    "join_keys = ['FL_DATE', 'OP_UNIQUE_CARRIER', 'TAIL_NUM', 'OP_CARRIER_FL_NUM', 'sched_depart_date_time_UTC', 'DEP_DEL15']\n",
    "df_flights_numeric_non_null = df_flights.select(join_keys + numerics_features).na.drop().cache()\n",
    "print(f\"Total Number of Flights (Non Null): {df_flights_numeric_non_null.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d65918e-9c11-4b54-8fe8-d9697d3c366a",
   "metadata": {},
   "source": [
    "# Features Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3f85fa9-23c8-421e-aa3f-15beb74484e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+--------+-----------------+--------------------------+---------+-------------------+------------------------+----------------------+----------------------+---------------------+----------------+---------------+---------+--------------------+\n",
      "|   FL_DATE|OP_UNIQUE_CARRIER|TAIL_NUM|OP_CARRIER_FL_NUM|sched_depart_date_time_UTC|DEP_DEL15|HourlyPrecipitation|HourlyDryBulbTemperature|HourlyRelativeHumidity|HourlySeaLevelPressure|HourlyStationPressure|HourlyVisibility|HourlyWindSpeed|ELEVATION|            features|\n",
      "+----------+-----------------+--------+-----------------+--------------------------+---------+-------------------+------------------------+----------------------+----------------------+---------------------+----------------+---------------+---------+--------------------+\n",
      "|2015-01-10|               AA|  N786AA|                2|       2015-01-10 17:00:00|      0.0|               0.01|                    58.0|                  81.0|                 29.97|                29.62|             3.0|            7.0|     29.6|[0.00999999977648...|\n",
      "|2015-03-24|               AA|  N789AA|                3|       2015-03-24 16:30:00|      0.0|                0.0|                    39.0|                  31.0|                 30.33|                 30.3|            10.0|           10.0|      3.4|[0.0,39.0,31.0,30...|\n",
      "|2015-01-23|               AA|  N381AA|                5|       2015-01-23 19:15:00|      0.0|                0.0|                    50.0|                  39.0|                 30.16|                29.52|            10.0|           22.0|    170.7|[0.0,50.0,39.0,30...|\n",
      "|2015-01-08|               AA|  N386AA|                8|       2015-01-09 03:45:00|      0.0|                0.0|                    71.0|                  71.0|                 30.01|                29.99|            10.0|            5.0|      2.1|[0.0,71.0,71.0,30...|\n",
      "|2015-02-15|               AA|  N376AA|                8|       2015-02-16 03:45:00|      0.0|                0.0|                    66.0|                  75.0|                 29.98|                29.96|            10.0|            6.0|      2.1|[0.0,66.0,75.0,29...|\n",
      "|2015-01-02|               AA|  N375AA|               23|       2015-01-02 18:50:00|      0.0|                0.0|                    52.0|                  53.0|                 30.13|                30.11|            10.0|            0.0|      2.4|[0.0,52.0,53.0,30...|\n",
      "|2015-01-13|               AA|  N385AA|               23|       2015-01-13 18:53:00|      0.0|                0.0|                    63.0|                  48.0|                 30.12|                 30.1|            10.0|            0.0|      2.4|[0.0,63.0,48.0,30...|\n",
      "|2015-03-30|               AA|  N3FLAA|               23|       2015-03-31 01:05:00|      0.0|                0.0|                    57.0|                  93.0|                 30.01|                29.67|             5.0|            9.0|     29.6|[0.0,57.0,93.0,30...|\n",
      "|2015-01-13|               AA|  N786AA|               24|       2015-01-13 15:00:00|      0.0|                0.0|                    58.0|                  78.0|                 30.17|                30.16|            10.0|            3.0|      2.4|[0.0,58.0,78.0,30...|\n",
      "|2015-02-02|               AA|  N4XKAA|               29|       2015-02-02 20:30:00|      0.0|                0.0|                    73.0|                  35.0|                 29.98|                29.03|            10.0|            8.0|    289.3|[0.0,73.0,35.0,29...|\n",
      "+----------+-----------------+--------+-----------------+--------------------------+---------+-------------------+------------------------+----------------------+----------------------+---------------------+----------------+---------------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "\n",
    "\n",
    "# build a vector assembler for the input features\n",
    "outputCol = \"features\"\n",
    "va = VectorAssembler(inputCols=numerics_features, outputCol=outputCol)\n",
    "\n",
    "# setting up a pipeline\n",
    "pipeline_stages = [va]\n",
    "feature_engineered_pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "## apply the pipeline to the train and test set\n",
    "fitted_pipeline = feature_engineered_pipeline.fit(df_flights_numeric_non_null)\n",
    "df_flights_numeric_non_null_VA = fitted_pipeline.transform(df_flights_numeric_non_null)\n",
    "df_flights_numeric_non_null_VA.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c2aab-f493-46c8-8594-45a4cd3ec4d2",
   "metadata": {},
   "source": [
    "# SMOTE Implementation\r\n",
    "\n",
    "The idea behind SMOTE is to introduce some level of randomness when creating synthetic data for the minority class. The machniery of SMOTE is performing many batches of k-nearest neighbours on the miniory class of data and by that, generate a new set of datapoint that follows the outputs neighbors. Typically SMOTE is a more robust way to upsample the data compared to traditional resampling technique. For our experiment, we modified the implementation described i [SMOTE implementation in PySpark](https://medium.com/@hwangdb/smote-implementation-in-pyspark-76ec4ffa2f1d)  to upsample our data using Spark's Vectorized UDF. However, we're still facing challenges with long runtime (> 10 hours) when applying SMOTE upsampling on a 3M dataset. We were able to drill down on the long training time problem by linking it to the large amount of data being shuffle between all cluster nodes when a paritioned window function is applied. Noted for this study, this is an area of improvement that we wanted more time to deep dive into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4039ad3-53e2-42bf-849a-cbe74f7b1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors,VectorUDT\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7323348-a8d6-47c6-becf-d09d3ea96bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare features and label to be use\n",
    "features = 'features'\n",
    "label = 'DEP_DEL15'\n",
    "\n",
    "# copy the dataset so that we don't accidently overwrite the original\n",
    "vectorized_sdf = df_flights_numeric_non_null_VA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2ae5095-1d1d-43bb-a994-5007f6ed652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE configurations\n",
    "smote_config = {\n",
    "        \"k\": 3,              # number of neighbors\n",
    "        \"multiplier\": 3,     # number of synthetic batches\n",
    "        \"seed\": 234,         # random seeds\n",
    "        \"bucketLength\": 3   # bucket length to control the probability of features being hashed into the same bucket\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f896276b-2446-4e5a-b6d8-8558089322db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|            datasetA|            datasetB| EuclideanDistance|\n",
      "+--------------------+--------------------+------------------+\n",
      "|{[0.0,-24.0,73.0,...|{1.0, [0.0,0.0,46...|48.759756944252494|\n",
      "|{[0.0,-24.0,73.0,...|{1.0, [0.0,8.0,80...|246.92416954269333|\n",
      "|{[0.0,-24.0,73.0,...|{1.0, [0.0,9.0,84...|262.96065139070686|\n",
      "|{[0.0,-24.0,73.0,...|{1.0, [0.0,10.0,4...|47.970058376160665|\n",
      "|{[0.0,-24.0,73.0,...|{1.0, [0.0,10.0,4...|45.206304874015046|\n",
      "|{[0.0,-24.0,73.0,...|{1.0, [0.0,11.0,4...| 46.63065300088498|\n",
      "|{[0.0,-24.0,73.0,...|{1.0, [0.0,11.0,6...|127.36071675040209|\n",
      "|{[0.0,-24.0,73.0,...|{1.0, [0.0,11.0,7...|200.95830414827935|\n",
      "|{[0.0,-24.0,73.0,...|{1.0, [0.0,11.0,7...|186.09608861742507|\n",
      "|{[0.0,-24.0,73.0,...|{1.0, [0.0,12.0,5...|183.10233072174813|\n",
      "+--------------------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the data by mininum and maximum\n",
    "# from earlier EDA, the minority class is 1, meaning the flight is delayed\n",
    "cols = [features, label]\n",
    "vectorized_sdf = vectorized_sdf.select(*cols).sample(fraction=0.10)\n",
    "dataInput_min = vectorized_sdf.filter(f\"{label} == '1'\")\n",
    "dataInput_maj = vectorized_sdf.filter(f\"{label} == '0'\")\n",
    "\n",
    "# LSH, bucketed random projection\n",
    "# Bucket Random Projection LSH calculates\n",
    "brp = BucketedRandomProjectionLSH(inputCol=features, outputCol=\"hashes\", seed=smote_config['seed'], bucketLength=smote_config['bucketLength'])\n",
    "\n",
    "# Applies SMOTE only on existing minority instances    \n",
    "model = brp.fit(dataInput_min)\n",
    "model.transform(dataInput_min)\n",
    "\n",
    "# here distance is calculated from brp's param inputCol\n",
    "self_join_w_distance = model.approxSimilarityJoin(dataInput_min, dataInput_min, float(\"inf\"), distCol=\"EuclideanDistance\")\n",
    "self_join_w_distance.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34b6daea-b29c-473a-9466-dcd1a4d75863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11863562"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_join_w_distance.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68b6b245-e2f5-41c8-9cd8-e1e63b462f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+-----+\n",
      "|            datasetA|            datasetB| EuclideanDistance|r_num|\n",
      "+--------------------+--------------------+------------------+-----+\n",
      "|{[0.0,-9.0,61.0,3...|{1.0, [0.0,16.0,4...| 65.90818300739144|    1|\n",
      "|{[0.0,-9.0,61.0,3...|{1.0, [0.0,0.0,69...|  77.5459902270549|    2|\n",
      "|{[0.0,-9.0,61.0,3...|{1.0, [0.0,1.0,68...| 77.57315386629878|    3|\n",
      "|{[0.0,-9.0,61.0,3...|{1.0, [0.0,74.0,1...|290.52299251928054|    4|\n",
      "|{[0.0,-9.0,61.0,3...|{1.0, [0.0,34.0,6...|   357.25361257248|    5|\n",
      "|{[0.0,-9.0,61.0,3...|{1.0, [0.0,36.0,6...|357.38525501022826|    6|\n",
      "|{[0.0,-9.0,61.0,3...|{1.0, [0.0,46.0,4...| 359.1979482089619|    7|\n",
      "|{[0.0,-9.0,61.0,3...|{1.0, [0.01999999...|359.40343807344306|    8|\n",
      "|{[0.0,-9.0,61.0,3...|{1.0, [0.0,49.0,3...| 360.1249158261858|    9|\n",
      "|{[0.0,-9.0,61.0,3...|{1.0, [0.0,52.0,4...| 360.3580927339047|   10|\n",
      "+--------------------+--------------------+------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove self-comparison (distance 0)\n",
    "self_join_w_distance = self_join_w_distance.filter(self_join_w_distance.EuclideanDistance > 0)\n",
    "\n",
    "over_original_rows = Window.partitionBy(\"datasetA\").orderBy(\"EuclideanDistance\")\n",
    "\n",
    "self_similarity_df = self_join_w_distance.withColumn(\"r_num\", F.row_number().over(over_original_rows))\n",
    "\n",
    "self_similarity_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "25721ce3-c4f9-45a2-8223-a349b044c4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating batch 0 of synthetic instances\n",
      "generating batch 1 of synthetic instances\n",
      "generating batch 2 of synthetic instances\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_282/496026766.py\", line 9, in <lambda>\nNameError: name 'random' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m oversampled_df \u001b[38;5;241m=\u001b[39m oversampled_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(F\u001b[38;5;241m.\u001b[39mrand())\n\u001b[1;32m     32\u001b[0m oversampled_df \u001b[38;5;241m=\u001b[39m oversampled_df\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m252\u001b[39m)\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m---> 33\u001b[0m \u001b[43moversampled_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_282/496026766.py\", line 9, in <lambda>\nNameError: name 'random' is not defined\n"
     ]
    }
   ],
   "source": [
    "self_similarity_df_selected = self_similarity_df.filter(self_similarity_df.r_num <= smote_config[\"k\"])\n",
    " \n",
    "over_original_rows_no_order = Window.partitionBy('datasetA')\n",
    "\n",
    "# list to store batches of synthetic data\n",
    "res = [dataInput_min, dataInput_maj]\n",
    "\n",
    "# two udf for vector add and subtract, subtraction include a random factor [0,1]\n",
    "subtract_vector_udf = F.udf(lambda arr: random.uniform(0, 1)*(arr[0]-arr[1]), VectorUDT())\n",
    "add_vector_udf = F.udf(lambda arr: arr[0]+arr[1], VectorUDT())\n",
    "\n",
    "\n",
    "# apply SMOTE iterations\n",
    "for i in range(smote_config['multiplier']):\n",
    "    print(\"generating batch %s of synthetic instances\"%i)\n",
    "    # logic to randomly select neighbour: pick the largest random number generated row as the neighbour\n",
    "    df_random_sel = self_similarity_df_selected.withColumn(\"rand\", F.rand()).withColumn('max_rand', F.max('rand').over(over_original_rows_no_order))\\\n",
    "                        .where(F.col('rand') == F.col('max_rand')).drop(*['max_rand','rand','r_num'])\n",
    "    # create synthetic feature numerical part\n",
    "    df_vec_diff = df_random_sel.select('*', subtract_vector_udf(F.array('datasetA.features', 'datasetB.features')).alias('vec_diff'))\n",
    "    df_vec_modified = df_vec_diff.select('*', add_vector_udf(F.array('datasetA.features', 'vec_diff')).alias('features'))\n",
    "    # df_vec_diff = df_random_sel.withColumn('vec_diff', subtract_vector_udf(F.col('')))\n",
    "    \n",
    "    # this df_vec_modified is the synthetic minority instances,\n",
    "    # df_vec_modified = df_vec_modified.drop(*['datasetA','datasetB','vec_diff','EuclideanDistance'])\n",
    "    df_vec_modified = df_vec_modified.select(features)\\\n",
    "                                    .withColumn('DEP_DEL15', F.lit(1))\n",
    "    res.append(df_vec_modified)\n",
    "\n",
    "oversampled_df = reduce(DataFrame.unionByName, res)\n",
    "oversampled_df = oversampled_df.select(\"*\").orderBy(F.rand())\n",
    "oversampled_df = oversampled_df.repartition(252).cache()\n",
    "oversampled_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289552b-bb57-4a28-9832-0109ccaf1284",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## spark smote oversampling ##########################\n",
    "#for categorical columns, must take its stringIndexed form (smote should be after string indexing, default by frequency)\n",
    " \n",
    "def smote(vectorized_sdf, features, label, smote_config):\n",
    "    '''\n",
    "    contains logic to perform smote oversampling, given a spark df with 2 classes\n",
    "    inputs:\n",
    "    * vectorized_sdf: cat cols are already stringindexed, num cols are assembled into 'features' vector\n",
    "      df target col should be 'label'\n",
    "    * smote_config: config obj containing smote parameters\n",
    "    output:\n",
    "    * oversampled_df: spark df after smote oversampling\n",
    "    '''\n",
    "    cols = [features, label]\n",
    "    vectorized_sdf = vectorized_sdf.select(*cols)\n",
    "    dataInput_min = vectorized_sdf.filter(f\"{label} == '1'\")\n",
    "    dataInput_maj = vectorized_sdf.filter(f\"{label} == '0'\")\n",
    "    \n",
    "    # LSH, bucketed random projection\n",
    "    brp = BucketedRandomProjectionLSH(inputCol=\"input_features\", outputCol=\"hashes\",seed=smote_config['seed'], bucketLength=smote_config['bucketLength'])\n",
    " \n",
    "    # smote only applies on existing minority instances    \n",
    "    model = brp.fit(dataInput_min)\n",
    "    model.transform(dataInput_min)\n",
    " \n",
    "    # here distance is calculated from brp's param inputCol\n",
    "    self_join_w_distance = model.approxSimilarityJoin(dataInput_min, dataInput_min, float(\"inf\"), distCol=\"EuclideanDistance\")\n",
    " \n",
    "    # remove self-comparison (distance 0)\n",
    "    self_join_w_distance = self_join_w_distance.filter(self_join_w_distance.EuclideanDistance > 0)\n",
    " \n",
    "    over_original_rows = Window.partitionBy(\"datasetA\").orderBy(\"EuclideanDistance\")\n",
    " \n",
    "    self_similarity_df = self_join_w_distance.withColumn(\"r_num\", F.row_number().over(over_original_rows))\n",
    " \n",
    "    self_similarity_df_selected = self_similarity_df.filter(self_similarity_df.r_num <= smote_config[\"k\"])\n",
    " \n",
    "    over_original_rows_no_order = Window.partitionBy('datasetA')\n",
    " \n",
    "    # list to store batches of synthetic data\n",
    "    res = [dataInput_min, dataInput_maj]\n",
    "    \n",
    "    # two udf for vector add and subtract, subtraction include a random factor [0,1]\n",
    "    subtract_vector_udf = F.pandas_udf(lambda arr: random.uniform(0, 1)*(arr[0]-arr[1]), VectorUDT())\n",
    "    add_vector_udf = F.pandas_udf(lambda arr: arr[0]+arr[1], VectorUDT())\n",
    " \n",
    " \n",
    "    # apply SMOTE iterations\n",
    "    for i in range(smote_config['multiplier']):\n",
    "        print(\"generating batch %s of synthetic instances\"%i)\n",
    "        # logic to randomly select neighbour: pick the largest random number generated row as the neighbour\n",
    "        df_random_sel = self_similarity_df_selected.withColumn(\"rand\", F.rand()).withColumn('max_rand', F.max('rand').over(over_original_rows_no_order))\\\n",
    "                            .where(F.col('rand') == F.col('max_rand')).drop(*['max_rand','rand','r_num'])\n",
    "        # create synthetic feature numerical part\n",
    "        df_vec_diff = df_random_sel.select('*', subtract_vector_udf(F.array('datasetA.input_features', 'datasetB.input_features')).alias('vec_diff'))\n",
    "        df_vec_modified = df_vec_diff.select('*', add_vector_udf(F.array('datasetA.input_features', 'vec_diff')).alias('input_features'))\n",
    "        # df_vec_diff = df_random_sel.withColumn('vec_diff', subtract_vector_udf(F.col('')))\n",
    "        \n",
    "        # this df_vec_modified is the synthetic minority instances,\n",
    "        # df_vec_modified = df_vec_modified.drop(*['datasetA','datasetB','vec_diff','EuclideanDistance'])\n",
    "        df_vec_modified = df_vec_modified.select(features)\\\n",
    "                                        .withColumn('DEP_DEL15', F.lit(1))\n",
    "        res.append(df_vec_modified)\n",
    "    \n",
    "    oversampled_df = reduce(DataFrame.unionByName, res)\n",
    "    oversampled_df = oversampled_df.select(\"*\").orderBy(F.rand())\n",
    "    oversampled_df = oversampled_df.repartition(252).cache()  # evenly distributed the shuffle data\n",
    "    # # union synthetic instances with original full (both minority and majority) df\n",
    "    # # oversampled_df = dfunion.union(vectorized_sdf.select(dfunion.columns))\n",
    "    \n",
    "    return oversampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea268b50-7599-4346-ba8f-8dca0cf3a4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
